{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"colab":{"provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"wDCRVtiXGS3k","outputId":"38795663-9aff-459b-86ad-bdcd8bbf1d23","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732310324736,"user_tz":360,"elapsed":1336,"user":{"displayName":"ETHAN XIA","userId":"12860508407677473150"}}},"source":["import requests\n","import pandas\n","from dateutil import parser\n","host = 'http://18.188.56.207:9200/'\n","requests.get(host + '_cat/indices/enron').content"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["b'yellow open enron lVq0is2BTCmgDk2kFyZHTQ 1 1 251735 115558 895.3mb 895.3mb\\n'"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"usPZo8sRGS3q","outputId":"7b7ed704-2f88-40ac-8f44-25714ee6c506","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732310331670,"user_tz":360,"elapsed":140,"user":{"displayName":"ETHAN XIA","userId":"12860508407677473150"}}},"source":["doc = {\n","    \"query\" : {\n","        \"match_all\" : {}\n","    }\n","}\n","import json\n","r=requests.get(host + 'enron/_search', data=json.dumps(doc), headers={'Content-Type':'application/json'})\n","print(len(r.json()['hits']['hits']))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["10\n"]}]},{"cell_type":"code","metadata":{"id":"L-lcMiQfGS3v"},"source":["def elasticsearch_results_to_df(results):\n","    '''\n","    A function that will take the results of a requests.get\n","    call to Elasticsearch and return a pandas.DataFrame object\n","    with the results\n","    '''\n","    hits = results.json()['hits']['hits']\n","    data = pandas.DataFrame([i['_source'] for i in hits], index = [i['_id'] for i in hits])\n","    data['date'] = data['date'].apply(parser.parse)\n","    return(data)\n","\n","def print_df_row(row):\n","    '''\n","    A function that will take a row of the data frame and print it out\n","    '''\n","    print('____________________')\n","    print('RE: %s' % row.get('subject',''))\n","    print('At: %s' % row.get('date',''))\n","    print('From: %s' % row.get('sender',''))\n","    print('To: %s' % row.get('recipients',''))\n","    print('CC: %s' % row.get('cc',''))\n","    print('BCC: %s' % row.get('bcc',''))\n","    print('Body:\\n%s' % row.get('text',''))\n","    print('____________________')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q7k93M9nGS30","outputId":"5698ab99-d0a3-4608-817e-c222487ddb76","colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"status":"error","timestamp":1732244431861,"user_tz":300,"elapsed":777,"user":{"displayName":"Lei Xue","userId":"10265392987030577048"}}},"source":["# Put elasticsearch results into a pandas.DataFrame object\n","# df = elasticsearch_results_to_df(r)\n","# print(df)\n","# print_df_row(df.iloc[0])"],"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"Parser must be a string or character stream, not float","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-0ffba3d7830f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Put elasticsearch results into a pandas.DataFrame object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melasticsearch_results_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint_df_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-900d5ad24459>\u001b[0m in \u001b[0;36melasticsearch_results_to_df\u001b[0;34m(results)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mhits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_source'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhits\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhits\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n","\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dateutil/parser/_parser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(timestr, parserinfo, **kwargs)\u001b[0m\n\u001b[1;32m   1366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparserinfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDEFAULTPARSER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dateutil/parser/_parser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, timestr, default, ignoretz, tzinfos, **kwargs)\u001b[0m\n\u001b[1;32m    638\u001b[0m                                                       second=0, microsecond=0)\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipped_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dateutil/parser/_parser.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(self, timestr, dayfirst, yearfirst, fuzzy, fuzzy_with_tokens)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_timelex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# Splits the timestr into tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0mskipped_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dateutil/parser/_parser.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(cls, s)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dateutil/parser/_parser.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, instream)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0minstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'read'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             raise TypeError('Parser must be a string or character stream, not '\n\u001b[0m\u001b[1;32m     70\u001b[0m                             '{itype}'.format(itype=instream.__class__.__name__))\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Parser must be a string or character stream, not float"]}]},{"cell_type":"code","metadata":{"id":"2teqoyC2GS34","outputId":"d45d787c-71de-4742-b38b-ddeb507d0d36","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732244434436,"user_tz":300,"elapsed":812,"user":{"displayName":"Lei Xue","userId":"10265392987030577048"}}},"source":["# Query For a full text match in the \"text\" field\n","# Uses the \"match\" query: https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-query.html\n","doc = {\n","    \"query\": {\n","        \"match\" : {\n","            \"text\" : \"important reporting\"\n","        }\n","    },\n","    \"from\" : 0, # Starting message to return.\n","    \"size\" : 2000, # Return this many messages. Can't be more than 10,000\n","}\n","r=requests.get(host + 'enron/_search',\n","               data=json.dumps(doc), headers={'Content-Type':'application/json'})\n","r.raise_for_status()\n","print(\"Found %s messages matching the query, of \" % r.json()['hits']['total'])\n","df = elasticsearch_results_to_df(r)\n","print(\"Returned %s messages\" % df.shape[0])\n","print_df_row(df.iloc[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found {'value': 10000, 'relation': 'gte'} messages matching the query, of \n","Returned 2000 messages\n","____________________\n","RE: RE: Reporting replication issue is now fixed\n","At: 2001-10-09 17:11:18+00:00\n","From: lynn.blair@enron.com\n","To: jennifer.lowry@enron.com  group.dl-ets@enron.com\n","CC: nan\n","BCC: nan\n","Body:\n","\tJennifer, how long has this been a problem?  Is there a concern we have caused\n","\tcustomers problems in nominating due to bad information?  Thanks. Lynn\n","\n"," -----Original Message-----\n","From: \tLowry, Jennifer   \n","Sent:\tTuesday, October 09, 2001 10:11 AM\n","To:\tDL-ETS TMS Modification Group\n","Subject:\tReporting replication issue is now fixed\n","\n","\n","Yesterday we noticed a problem where reports were not reporting on the correct cycle, or were not picking up information between cycles.  As it turns out, an important table was not being replicated from the application database to the reporting database.  \n","\n","I was told that this problem has been fixed, and on first inspection of the tables, everything looks correct.\n","____________________\n"]}]},{"cell_type":"code","metadata":{"id":"UbzY2Pg5GS38","outputId":"8b6cabf5-981a-4758-b41f-61a63943059a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732244440255,"user_tz":300,"elapsed":149,"user":{"displayName":"Lei Xue","userId":"10265392987030577048"}}},"source":["# Query For a text match in the \"text\" or \"subject\" fields. Uses the multi-match query:\n","# https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-multi-match-query.html\n","doc = {\n","  \"query\": {\n","    \"multi_match\" : {\n","      \"query\":    \"settlement\",\n","      \"fields\": [ \"subject\", \"text\" ]\n","    }\n","  }\n","}\n","r=requests.get(host + 'enron/_search',\n","               data=json.dumps(doc), headers={'Content-Type':'application/json'})\n","r.raise_for_status()\n","print(\"Found %s messages matching the query, of \" % r.json()['hits']['total'])\n","df = elasticsearch_results_to_df(r)\n","print(\"Returned %s messages\" % df.shape[0])\n","print_df_row(df.iloc[9])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found {'value': 4102, 'relation': 'eq'} messages matching the query, of \n","Returned 10 messages\n","____________________\n","RE: Status of final statements and replacement invoices\n","At: 2001-10-24 01:34:50+00:00\n","From: thailu@ercot.com\n","To: jackson.amie@enron.com  pratka.amy@enron.com  mitrey.andy@enron.com  williams.angela@enron.com  garza.beth@enron.com  palmer.bill@enron.com  cooper.bob@enron.com  edwards.brady@enron.com  green.brenda@enron.com  smith.carl@enron.com  smith.carl@enron.com  carey.dan@enron.com  sarti.dan@enron.com  leger.dana@enron.com  wessels.david@enron.com  pawlik.debbie@enron.com  bailey.debra@enron.com  dyc.dennis@enron.com  slover.eric@enron.com  nitschmann.frances@enron.com  herndon.gary@enron.com  geissler.ginger@enron.com  striedel.james@enron.com  holland.janet@enron.com  jeffrey.miller@enron.com  doyas.jenny@enron.com  burt.jerry@enron.com  barker.joe@enron.com  favalora.joe@enron.com  forney.john@enron.com  fitzmaurice.kathy@enron.com  minear.kelly@enron.com  koliba.kim@enron.com  star.lee@enron.com  master.linda@enron.com  jones.lisa@enron.com  ramirez.liz@enron.com  kinner.lora@enron.com  rubenstein.marc@enron.com  colby.maria@enron.com  gardner.maria@enron.com  deane.marianne@enron.com  nealy.marice@enron.com  wright.mark@enron.com  murphy.martha@enron.com  bulk.max@enron.com  michael.jacobson@enron.com  coyle.mike@enron.com  tejada.mike@enron.com  vogelei.pat@enron.com  krebs.paul@enron.com  paul.radous@enron.com  pavluk.peter@enron.com  robinson.phillip@enron.com  ganesh.ramesh@enron.com  hager.rhonda@enron.com  rucker.rick@enron.com  vick.rick@enron.com  williamson.rick@enron.com  day.rosalie@enron.com  davis.sam@enron.com  eubank.sandra@enron.com  wagner.shannon@enron.com  mcknight.shonda@enron.com  francois.sonya@enron.com  schamel.stacy@enron.com  mullen.stephanie@enron.com  potters.susan@enron.com  nikazm.tamila@enron.com  ho.thomas@enron.com  a..allen@enron.com  white.tom@enron.com  martin.troy@enron.com  petrov.valentin@enron.com  kuhn.walt@enron.com\n","CC: clientreps@ercot.com  settlebilling@ercot.com  clientreps@ercot.com  grendel@enron.com  --migrated--sgrendel@ercot.com  bojorquez@enron.com  bbojorquez@ercot.com  noel@enron.com  tnoel@ercot.com  jones@enron.com  sjones@ercot.com  tindall@enron.com  htindall@ercot.com  scsleads@ercot.com  buckles@enron.com  mbuckles@ercot.com  saathoff@enron.com  ksaathoff@ercot.com\n","BCC: clientreps@ercot.com  settlebilling@ercot.com  clientreps@ercot.com  grendel@enron.com  --migrated--sgrendel@ercot.com  bojorquez@enron.com  bbojorquez@ercot.com  noel@enron.com  tnoel@ercot.com  jones@enron.com  sjones@ercot.com  tindall@enron.com  htindall@ercot.com  scsleads@ercot.com  buckles@enron.com  mbuckles@ercot.com  saathoff@enron.com  ksaathoff@ercot.com\n","Body:\n","\n","This is an update on publication of QSE settlement invoices and statements as of today, October 23, 2001.  ERCOT has published a replacement invoice to add final settlement statements to the previously issued invoice with a due date of 10/29/01.  This invoice was previously published with initial settlement statements for trade days of 09/14/01 through 09/20/01.  The replacement invoice adds final settlement statements for trade days 08/03/01 through 08/09/01 to this invoice.  The replacement invoice was posted to the portal on 10/18/01.  \n","ERCOT has also published a replacement invoice to add final settlement statements to the previously issued invoice with a due date of 11/05/01.  This invoice was previously published with initial settlement statements for trade days of  09/21/01 through 09/27/01 .  The replacement invoice adds final settlement statements for trade days 08/10/01 through 08/16/01 to this invoice. The replacement invoice was posted to the portal today.  \n","Please update your records to replace the previously issued invoices and process any payments due ERCOT as per the replacement invoices.  ERCOT will process the any payments due to QSEs as per the replacement invoices.\n","Final settlement statements through 08/16 are currently posted. \n","Ted  Hailu \n","Client Relations Manager \n","Electric Reliability Council of Texas \n","E-mail: Thailu@ercot.com \n","Telephone: (512) 248-3873 \n","____________________\n"]}]},{"cell_type":"code","metadata":{"id":"K87Pbc9KGS3_","outputId":"bfc24bfd-d5a6-4973-c4aa-adb7372c6647","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732244441689,"user_tz":300,"elapsed":161,"user":{"displayName":"Lei Xue","userId":"10265392987030577048"}}},"source":["# \"OR\" query for two phrase matches. Generally you get fancy query parsing with this:\n","# https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html\n","doc = {\n","    \"query\": {\n","        \"query_string\" : {\n","            \"default_field\" : \"text\",\n","            \"query\" : \"(reach a settlement) OR (continue to pursue)\"\n","        }\n","    }\n","}\n","r=requests.get(host + 'enron/_search',\n","               data=json.dumps(doc), headers={'Content-Type':'application/json'})\n","r.raise_for_status()\n","print(\"Found %s messages matching the query\" % r.json()['hits']['total']['value'])\n","df = elasticsearch_results_to_df(r)\n","print(\"Returned %s messages\" % df.shape[0])\n","print_df_row(df.iloc[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 10000 messages matching the query\n","Returned 10 messages\n","____________________\n","RE: Settlement Conversation Recap\n","At: 2002-05-02 18:26:52+00:00\n","From: michael.bridges@enron.com\n","To: koikosp@talgov.com\n","CC: chris.germany@enron.com\n","BCC: chris.germany@enron.com\n","Body:\n","Hi Pete,\n","\n","Wanted to recap our conversation yesterday, update you on our timing and get you my contact information.\n","\n","Regarding a possible settlement, it appears to me that this is a concept that you have considered and are willing to pursue.  We will have a valuation of the contract for you Monday.  It is my suggestion that you review the proposal, make sure that you agree with the outstanding transactions and complete your own valuation.  Once we have agreed on the universe of transactions, you and I will discuss and finalize a settlement amount that we are comfortable will be approved by the creditor committee and bankruptcy judge.\n","\n","From this point, Enron will begin the process of filing a motion for settlement and schedule an appropriate hearing date for the judge to rule.  Please seek bankruptcy council to determine if it will be necessary for a representative from your company to attend and, as well, to be comfortable with the process.\n","\n","Finally, we will provide you a sample Release Letter for your review.  We will seek your comments, if any, prior to the hearing date.  After the settlement motion is filed, we would ask that there are no more document or value negotiations in order to expedite closure.\n","\n","All of that said, concurrently, in case you and I are not able to reach agreement on a settlement amount, we are going to take the first step in an auction process.  This means that I am going to send you a procedural Notice of an anticipated auction to sell your contract as we discussed yesterday.  It is our desire to settle this prior to an auction taking place.  The settlement process is our preferred route of resolution.  Please call me if you have any questions regarding this.\n","\n","Please, call me with any questions and let's work hard at keeping an open dialog so that we can resolve this to both of our mutual benefits.  I have included my contact information at the end of this email-- if you could return this email with your mailing address, I would appreciate it.  Also, I look forward to working with you through this process, and finally, thank you for your time and attention to this matter.\n","\n","Sincerely,\n","\n","Mike Bridges\n","\n","\n","\n","Mike Bridges\n","phone:  713-345-4079\n","fax:\t713-646-3037\n","email:\tmichael.bridges@enron.com\n","\n","1400 Smith Street\n","ECN 0602d\n","Houston, TX 77002\n","____________________\n"]}]},{"cell_type":"code","metadata":{"id":"x8IudAe6GS4D","outputId":"70dacf9d-28ba-4070-fd6b-b6bb264b6915","colab":{"base_uri":"https://localhost:8080/","height":303},"executionInfo":{"status":"ok","timestamp":1732244444501,"user_tz":300,"elapsed":143,"user":{"displayName":"Lei Xue","userId":"10265392987030577048"}}},"source":["df.iloc[9]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["sender                                    w..cantrell@enron.com\n","recipients    d..steffes@enron.com  leslie.lawner@enron.com ...\n","cc                                                          NaN\n","text          \\nNGI's Daily Gas Price Index \\npublished : Au...\n","bcc                                                         NaN\n","date                                  2001-08-31 14:44:25+00:00\n","subject       NGI Article:  ALJ Advocates Money Settlement i...\n","Name: 95720, dtype: object"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>95720</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>sender</th>\n","      <td>w..cantrell@enron.com</td>\n","    </tr>\n","    <tr>\n","      <th>recipients</th>\n","      <td>d..steffes@enron.com  leslie.lawner@enron.com ...</td>\n","    </tr>\n","    <tr>\n","      <th>cc</th>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>text</th>\n","      <td>\\nNGI's Daily Gas Price Index \\npublished : Au...</td>\n","    </tr>\n","    <tr>\n","      <th>bcc</th>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>date</th>\n","      <td>2001-08-31 14:44:25+00:00</td>\n","    </tr>\n","    <tr>\n","      <th>subject</th>\n","      <td>NGI Article:  ALJ Advocates Money Settlement i...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> object</label>"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"SYNs9pQNGS4F","outputId":"74b3318d-e380-44b5-89b9-973465c9b24a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732244446302,"user_tz":300,"elapsed":307,"user":{"displayName":"Lei Xue","userId":"10265392987030577048"}}},"source":["# Do a count of all documents in the database by month\n","doc = {\n","    \"aggs\" : {\n","        \"aggregation_var_name\" : {\n","            \"date_histogram\" : {\n","                \"field\" : \"date\",\n","                \"interval\" : \"month\"\n","            }\n","        }\n","    }\n","}\n","r=requests.get(host + 'enron/_search',\n","               data=json.dumps(doc), headers={'Content-Type':'application/json'})\n","r.raise_for_status()\n","def aggregation_to_df(response,var_name='aggregation_var_name'):\n","    r = response.json()\n","    r['aggregations'][var_name]\n","    df = pandas.DataFrame(r['aggregations'][var_name]['buckets'])\n","    df['date'] = df['key_as_string'].apply(parser.parse)\n","    df = df[(df['date'] >= '1999-01-1') & (df['date'] < '2002-07-01')]\n","    df = df[['date','doc_count']]\n","    return df\n","df = aggregation_to_df(r)\n","print(df)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                         date  doc_count\n","228 1999-01-01 00:00:00+00:00         65\n","229 1999-02-01 00:00:00+00:00         43\n","230 1999-03-01 00:00:00+00:00         50\n","231 1999-04-01 00:00:00+00:00         45\n","232 1999-05-01 00:00:00+00:00        338\n","233 1999-06-01 00:00:00+00:00        327\n","234 1999-07-01 00:00:00+00:00        446\n","235 1999-08-01 00:00:00+00:00        509\n","236 1999-09-01 00:00:00+00:00        588\n","237 1999-10-01 00:00:00+00:00        643\n","238 1999-11-01 00:00:00+00:00        594\n","239 1999-12-01 00:00:00+00:00       1248\n","240 2000-01-01 00:00:00+00:00       2142\n","241 2000-02-01 00:00:00+00:00       2471\n","242 2000-03-01 00:00:00+00:00       3001\n","243 2000-04-01 00:00:00+00:00       2991\n","244 2000-05-01 00:00:00+00:00       3718\n","245 2000-06-01 00:00:00+00:00       4820\n","246 2000-07-01 00:00:00+00:00       4493\n","247 2000-08-01 00:00:00+00:00       6200\n","248 2000-09-01 00:00:00+00:00       6872\n","249 2000-10-01 00:00:00+00:00       8273\n","250 2000-11-01 00:00:00+00:00      10617\n","251 2000-12-01 00:00:00+00:00       9989\n","252 2001-01-01 00:00:00+00:00       8212\n","253 2001-02-01 00:00:00+00:00       8387\n","254 2001-03-01 00:00:00+00:00      10503\n","255 2001-04-01 00:00:00+00:00      13726\n","256 2001-05-01 00:00:00+00:00      16845\n","257 2001-06-01 00:00:00+00:00      11141\n","258 2001-07-01 00:00:00+00:00       7353\n","259 2001-08-01 00:00:00+00:00       8044\n","260 2001-09-01 00:00:00+00:00       8963\n","261 2001-10-01 00:00:00+00:00      27691\n","262 2001-11-01 00:00:00+00:00      22077\n","263 2001-12-01 00:00:00+00:00       9535\n","264 2002-01-01 00:00:00+00:00      16341\n","265 2002-02-01 00:00:00+00:00       6484\n","266 2002-03-01 00:00:00+00:00       2875\n","267 2002-04-01 00:00:00+00:00        951\n","268 2002-05-01 00:00:00+00:00        748\n","269 2002-06-01 00:00:00+00:00        744\n"]}]},{"cell_type":"code","metadata":{"id":"3T5IWPrAGS4J","outputId":"9d0f66f8-0040-4db5-a64a-03d28611eecd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732244450663,"user_tz":300,"elapsed":174,"user":{"displayName":"Lei Xue","userId":"10265392987030577048"}}},"source":["# Do a count of all documents matching a query by month\n","doc = {\n","    \"query\": {\n","        \"match\" : {\n","            \"text\" : \"important reporting\"\n","        }\n","    },\n","    \"aggs\" : {\n","        \"aggregation_var_name\" : {\n","            \"date_histogram\" : {\n","                \"field\" : \"date\",\n","                \"interval\" : \"month\"\n","            }\n","        }\n","    }\n","}\n","r=requests.get(host + 'enron/_search',\n","               data=json.dumps(doc), headers={'Content-Type':'application/json'})\n","r.raise_for_status()\n","df = aggregation_to_df(r)\n","print(df)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                         date  doc_count\n","228 1999-01-01 00:00:00+00:00         12\n","229 1999-02-01 00:00:00+00:00          7\n","230 1999-03-01 00:00:00+00:00          2\n","231 1999-04-01 00:00:00+00:00          8\n","232 1999-05-01 00:00:00+00:00         12\n","233 1999-06-01 00:00:00+00:00         13\n","234 1999-07-01 00:00:00+00:00         21\n","235 1999-08-01 00:00:00+00:00         22\n","236 1999-09-01 00:00:00+00:00         14\n","237 1999-10-01 00:00:00+00:00         15\n","238 1999-11-01 00:00:00+00:00         31\n","239 1999-12-01 00:00:00+00:00         55\n","240 2000-01-01 00:00:00+00:00         89\n","241 2000-02-01 00:00:00+00:00        104\n","242 2000-03-01 00:00:00+00:00        116\n","243 2000-04-01 00:00:00+00:00         92\n","244 2000-05-01 00:00:00+00:00        157\n","245 2000-06-01 00:00:00+00:00        176\n","246 2000-07-01 00:00:00+00:00        180\n","247 2000-08-01 00:00:00+00:00        268\n","248 2000-09-01 00:00:00+00:00        312\n","249 2000-10-01 00:00:00+00:00        403\n","250 2000-11-01 00:00:00+00:00        625\n","251 2000-12-01 00:00:00+00:00        517\n","252 2001-01-01 00:00:00+00:00        325\n","253 2001-02-01 00:00:00+00:00        345\n","254 2001-03-01 00:00:00+00:00        407\n","255 2001-04-01 00:00:00+00:00        587\n","256 2001-05-01 00:00:00+00:00       1014\n","257 2001-06-01 00:00:00+00:00        654\n","258 2001-07-01 00:00:00+00:00        429\n","259 2001-08-01 00:00:00+00:00        397\n","260 2001-09-01 00:00:00+00:00        523\n","261 2001-10-01 00:00:00+00:00       1711\n","262 2001-11-01 00:00:00+00:00       1286\n","263 2001-12-01 00:00:00+00:00        496\n","264 2002-01-01 00:00:00+00:00        791\n","265 2002-02-01 00:00:00+00:00        334\n","266 2002-03-01 00:00:00+00:00         90\n","267 2002-04-01 00:00:00+00:00         17\n","268 2002-05-01 00:00:00+00:00         26\n","269 2002-06-01 00:00:00+00:00         20\n"]}]},{"cell_type":"code","metadata":{"id":"inNJYYxIGS4M","outputId":"237f3c9a-37cd-47ee-91c5-f3365b56ae64","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732244453163,"user_tz":300,"elapsed":137,"user":{"displayName":"Lei Xue","userId":"10265392987030577048"}}},"source":["# Search an exact match in a specific feild\n","doc = {\n","    \"query\": {\n","        \"match\" : {\n","            \"recipients\" : \"stephen.schwarzbach@enron.com\"\n","        }\n","    },\n","}\n","r=requests.get(host + 'enron/_search',\n","               data=json.dumps(doc), headers={'Content-Type':'application/json'})\n","r.raise_for_status()\n","print(\"Found %s messages matching the query, of \" % r.json()['hits']['total']['value'])\n","df = elasticsearch_results_to_df(r)\n","print(\"Returned %s messages\" % df.shape[0])\n","print_df_row(df.iloc[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 16 messages matching the query, of \n","Returned 10 messages\n","____________________\n","RE: RE: REQUEST - 2001 Operating Expenses - High Priority\n","At: 2001-10-02 22:52:12+00:00\n","From: tracy.geaccone@enron.com\n","To: stephen.schwarzbach@enron.com\n","CC: nan\n","BCC: nan\n","Body:\n","No PGG?\n","\n"," -----Original Message-----\n","From: \tSchwarzbach, Stephen  \n","Sent:\tTuesday, October 02, 2001 9:51 AM\n","To:\tGeaccone, Tracy; Tran, Agatha; Killen, Faith; Lee, Dana; Smith, Stephanie; Davis, Rachel A.; Vargas, Hope; Evans, Stephen; Sparger, Bob; Smith, Jeff E.; Myers, Thomas; Ueckert, Allen W.; Copeland, Erin; Selzer, Howard; Boyd, Mike; Smith, Kirk; Roper, Kerry; Galvan, Michael S.; Wood, Stephen; West, Terry\n","Cc:\tLindsey, Mark E.\n","Subject:\tREQUEST - 2001 Operating Expenses - High Priority\n","\n","Hello everyone.  I know that most, if not all, of you are busy with quarter close, 2002 Plan, and possibly Board of Directors presentations and that this is not the ideal time to have more added to your plate.  However, in conjunction to with setting the 2002 Plan targets, the Office of the Chairman has asked to see an analysis of 2001 operating expenses by business unit, with some level of detail, and with quick turnaround.\n","\n","To that end, and to expedite this request in an efficient manner (and hopefuly save you some work), corporate accounting has already prepared a summary of operating expenses by business unit by category for the eight months ended August 31, 2001.  These categories correlate very closely with the most often used accounts in SAP (cost element groups), i.e. salaries, benefits, outside services, etc.  This information ties to your rollup in CS.  Some categories have many accounts mapped to them, and we have provided the mapping employed by corporate for you to follow (column AO).\n","\n","What we are asking you to do is as follows:\n","\n","1. Review the attached schedule for the eight months ended 8/31/2001 (first sheet - Expenses-YTD August 2001).  Corporate has highlighted, in yellow, various amounts that warrant some additional explanation.  Please provide an explanation for these highlighted amounts.  You may do so in any format you choose (at the bottom with a footnote, additional schedule, etc.).\n","\n","2. As you may remember doing earlier this year with the 2001 Plan, please take your total expenses YTD 8/31/01 and categorize them by function.  We have provided a list of the most common functions in this file, but if you need to add functions for clarity, please do so.  Corporate gave thought to using the SAP alternate hierarchies to do this, but concluded that each BU should have input on this as you will know the most appropriate hierarchies to use and any other adjustments to make, if needed.\n","\n","3. Considering what has been incurred through August and what the remainder of the year holds for your BU, please project your total 2001 operating expenses by both category and by function.  Please fill in the second sheet in the file (Expenses-Sep-Dec 2001) with your projections and review the totals sheet (Total Year Estimate).  \n","\n","Keep in mind that this exercise is meant to be your best estimate at this time of these items, given the quick turnaround time, and not an exhaustive project. We realize that this is the best estimate at this time and understand that these numbers may change for a variety of reasons.\n","\n","If you have any questions, please call me at x33341 or Terry West at x36910.\n","\n","And finally, this needs to be completed and returned to me by 1:00pm this Wednesday, October 3.\n","\n","Thank you,\n","Steve\n","\n"," << File: Aug 2001 G&A est.xls >> \n","____________________\n"]}]},{"cell_type":"code","metadata":{"id":"FySeID6rGS4O"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# start of Lei Xue Part"],"metadata":{"id":"BjpCUyYkzQ8W"}},{"cell_type":"code","source":["# 那个CFO 的email list 看看？ 竟然才7条，可恶！！\n","doc = {\n","    \"query\": {\n","        \"multi_match\": {\n","            \"query\": \"andrew.fastow@enron.com\",\n","            \"fields\": [\"to\", \"cc\", \"bcc\", \"recipients\"]\n","        }\n","    }\n","}\n","r=requests.get(host + 'enron/_search',\n","               data=json.dumps(doc), headers={'Content-Type':'application/json'})\n","r.raise_for_status()\n","print(\"Found %s messages matching the query, of \" % r.json()['hits']['total']['value'])\n","df = elasticsearch_results_to_df(r)\n","print(\"Returned %s messages\" % df.shape[0])\n","print_df_row(df.iloc[0])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3g_02A5uzer8","executionInfo":{"status":"ok","timestamp":1732244699715,"user_tz":300,"elapsed":153,"user":{"displayName":"Lei Xue","userId":"10265392987030577048"}},"outputId":"2ffe34fd-65d0-49aa-f5d1-443a2651c9f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 7 messages matching the query, of \n","Returned 7 messages\n","____________________\n","RE: RE: The National Forum on Corporate Finance\n","At: 2001-02-26 11:20:00+00:00\n","From: vince.kaminski@enron.com\n","To: andrew.fastow@enron.com\n","CC: daveike@rice.edu  vince.kaminski@enron.com\n","BCC: daveike@rice.edu  vince.kaminski@enron.com\n","Body:\n","Andy,\n","\n"," Thanks. I shall forward  your message to \n","Prof. Ikenberry.\n","\n","Vince\n","\n","\n","\n","\n","From: Andrew S Fastow/ENRON@enronXgate on 02/22/2001 01:45 PM\n","To: Vince J Kaminski/HOU/ECT@ECT\n","cc:  \n","Subject: RE: The National Forum on Corporate Finance\n","\n","Vince:\n","\n","I would be interested in participating.  Thanks.\n","\n","Andy\n","\n"," -----Original Message-----\n","From:  Kaminski, Vince  \n","Sent: Monday, February 05, 2001 10:23 AM\n","To: Andrew S Fastow/HOU/ECT@ENRON\n","Cc: Kaminski, Vince\n","Subject: The National Forum on Corporate Finance\n","\n","Andy,\n","\n","I am sending you a draft oof a proposal regarding national forum for top \n","finance practitioners and\n","academics. The idea came from a professor at Rice University who\n","has already received a commitment from a number\n","of most distinguished CFOs. \n","\n","Please, read the outline and see if you would be interested in joining this \n","forum.\n","I shall be glad to help to arrange  a meeting with Prof. Ikenberry.\n","\n","Vince\n","\n","\n","---------------------- Forwarded by Vince J Kaminski/HOU/ECT on 02/05/2001 \n","10:22 AM ---------------------------\n","\n","\n","David Ikenberry <daveike@rice.edu> on 02/02/2001 06:10:02 PM\n","To: \"vkamins@ennron.com\" <vkamins@enron.com>\n","cc:  \n","Subject: \n","\n","\n","It was great talking with you.\n","\n","Dave \n"," - brochure.doc << File: brochure.doc >> \n","\n","\n","***********************************\n","Prof. David Ikenberry\n","Jones Graduate School of Management\n","Rice University\n","713-348-5385\n","\n","\n","\n","\n","\n","____________________\n"]}]},{"cell_type":"code","source":["mapping = requests.get(host + 'enron/_mapping').json()\n","print(mapping)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NDSRTjTw11K5","executionInfo":{"status":"ok","timestamp":1732244888196,"user_tz":300,"elapsed":151,"user":{"displayName":"Lei Xue","userId":"10265392987030577048"}},"outputId":"7c303e08-6ebc-4f41-c1fe-0cd23a4fee74"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'enron': {'mappings': {'properties': {'bcc': {'type': 'keyword'}, 'body': {'type': 'text', 'analyzer': 'english'}, 'cc': {'type': 'keyword'}, 'date': {'type': 'date'}, 'recipients': {'type': 'keyword'}, 'scroll': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}, 'scroll_id': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}, 'sender': {'type': 'keyword'}, 'subject': {'type': 'text', 'analyzer': 'english'}, 'text': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}}}}}\n"]}]},{"cell_type":"code","source":["# 关键词列表查找，发现了10000多条，可恶，我然后加入了时间搜查, 00-02的结果还是有1万+条\n","keywords = [\n","    \"Revenue Recognition\", \"Special Purpose Entity\", \"Off-Balance-Sheet\",\n","    \"Derivatives\", \"Mark-to-Market Accounting\", \"Internal Audit\",\n","    \"Compliance\", \"Risk Management\", \"Code of Ethics\",\n","    \"Accounting Treatment\", \"Financial Statements\", \"Asset Restructuring\",\n","    \"Capital Transactions\", \"SEC\", \"Investigation\", \"Litigation\",\n","    \"Compliance Review\", \"Executive Meeting\", \"Board of Directors\",\n","    \"Financial Reporting\", \"Disclosure\"\n","]\n","\n","doc = {\n","    \"query\": {\n","        \"bool\": {\n","            \"must\": [\n","                {\n","                    \"query_string\": {\n","                        \"query\": \" OR \".join(keywords),\n","                        \"fields\": [\"subject\"]\n","                    }\n","                },\n","                {\n","                    \"range\": {\n","                        \"date\": {\n","                            \"gte\": \"2000-01-01\",\n","                            \"lte\": \"2002-12-31\"\n","                        }\n","                    }\n","                }\n","            ]\n","        }\n","    },\n","}\n","r=requests.get(host + 'enron/_search',\n","               data=json.dumps(doc), headers={'Content-Type':'application/json'})\n","r.raise_for_status()\n","print(\"Found %s messages matching the query, of \" % r.json()['hits']['total']['value'])\n","df = elasticsearch_results_to_df(r)\n","print(\"Returned %s messages\" % df.shape[0])\n","print_df_row(df.iloc[0])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"id":"Pp7Xbzr51V4l","executionInfo":{"status":"error","timestamp":1732245275662,"user_tz":300,"elapsed":125,"user":{"displayName":"Lei Xue","userId":"10265392987030577048"}},"outputId":"0d852649-560c-4294-bede-6b8b4ddb970e"},"execution_count":null,"outputs":[{"output_type":"error","ename":"HTTPError","evalue":"400 Client Error: Bad Request for url: http://18.188.56.207:9200/enron/_search","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-940c3e7307fd>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m r=requests.get(host + 'enron/_search',\n\u001b[1;32m     38\u001b[0m                data=json.dumps(doc), headers={'Content-Type':'application/json'})\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Found %s messages matching the query, of \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melasticsearch_results_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: http://18.188.56.207:9200/enron/_search"]}]},{"cell_type":"markdown","source":["# End of Lei Xue Part"],"metadata":{"id":"p6mKymwyzYMC"}},{"cell_type":"markdown","source":["**Start** of Yi Xia Part\n","\n","不太确定如何把所有的email放dataframe里面。我现在local的email里面找了一下。\n","跑下面的code需要先download email。 下载的邮件一共是9万多封\n","```\n","# This is formatted as code\n","```\n","\n"],"metadata":{"id":"FwHzgI68Rw0V"}},{"cell_type":"markdown","source":["# Start of Yi Xia Part"],"metadata":{"id":"GKMCmmqNoZFO"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","from email import policy\n","from email.parser import BytesParser\n","from nltk.tokenize import word_tokenize\n","import nltk\n","\n","folder_path = r\"C:\\Users\\goxia\\OneDrive\\!INTA6450DAS\\GroupProject\\inta6450-emails\\dataset\"\n","\n","email_data = []\n","\n","import json\n","\n","for filename in os.listdir(folder_path):\n","    if filename.endswith('.json'):  # assuming the email files are in .json format\n","        file_path = os.path.join(folder_path, filename)\n","\n","        # Open and parse the JSON file\n","        with open(file_path, 'r', encoding='utf-8') as file:\n","            email = json.load(file)\n","\n","        # Extract the email text and subject (from JSON structure)\n","        text = email.get('text', 'No Text')\n","        subject = email.get('subject', 'No Subject')\n","\n","        # Add the email's subject and body to the list\n","        email_data.append({'subject': subject, 'body': text})\n","\n","df = pd.DataFrame(email_data)\n","print(df.head())\n","\n","print(df.iloc[3]['body'])\n","print(df.iloc[3]['subject'])\n","\n","df['tokenized_body'] = df['body'].apply(lambda x: word_tokenize(x.lower()))\n","\n","df['tokenized_body'] = df['tokenized_body'].apply(lambda tokens: list(dict.fromkeys(tokens)))\n","\n","print(df.head())\n","\n","print(df.iloc[3]['tokenized_body'])\n","\n","nltk.download('stopwords')\n","\n","from nltk.stem.porter import PorterStemmer\n","from nltk.corpus import stopwords\n","\n","corpus = {}\n","\n","stemmer = PorterStemmer()\n","\n","stopwords_set = set(stopwords.words('english'))\n","\n","for i in range(len(df)):\n","    text = df.iloc[i]['tokenized_body']\n","\n","    for j in range(len(text)):\n","        tk = text[j]\n","        if tk not in corpus.keys():\n","            corpus[text[j]] = 1\n","        else:\n","            corpus[text[j]] += 1\n","\n","# print(corpus)\n","key_to_print = 'requirement'\n","\n","value = corpus.get(key_to_print)\n","\n","print(len(corpus))\n","\n","corpus_short = {}\n","\n","signs = ['requirement','stock', 'price', 'market', 'transaction', 'buy', 'sell', 'shares', 'money', 'short', 'long', 'put']\n","\n","for i in signs:\n","    try:\n","        value = corpus.get(i)\n","        print(value)\n","    except:\n","        print('not found')\n","\n","# 847\n","# 2643\n","# 7369\n","# 7986\n","# 3192\n","# 3893\n","# 3056\n","# 1059\n","# 3761\n","# 3226\n","# 5034\n","# 5339\n","\n","count = 0\n","tf = pd.DataFrame()\n","result = []\n","\n","for i in range(len(df)):\n","    k = 0\n","    tk = df.iloc[i]['tokenized_body']\n","    for j in tk:\n","        if j in signs:\n","            k+=1\n","        if k>11:\n","            count+=1\n","            result.append({'index': i, 'body': df.iloc[i]['body']})\n","#           print(df.iloc[i]['body'])\n","\n","tf = pd.DataFrame(result)\n","print(count)\n","\n","#14283\n","\n","count = 0\n","sf = pd.DataFrame()\n","result = []\n","\n","signs2 = ['valuation']\n","\n","\n","for i in range(len(df)):\n","    k = 0\n","    tk = df.iloc[i]['tokenized_body']\n","#     if k < 5:\n","#         print(tk)\n","#     k+=1\n","    for j in tk:\n","        if j in signs2:\n","\n","            result.append({'index': i, 'body': df.iloc[i]['body']})\n","            count+=1\n","            continue\n","#           print(df.iloc[i]['body'])\n","\n","sf = pd.DataFrame(result)\n","print(count)\n","\n","#681\n","\n","for i in range(10):\n","    print(sf.iloc[i]['body'])\n","\n","# Example email found by above method:\n","\n","# Stinson Gibner\n","# 06/29/2000 09:55 AM\n","# To: Vince J Kaminski/HOU/ECT@ECT\n","# cc:\n","# Subject: EBS VaR Transaction Policy\n","\n","# FYI\n","\n","# ---------------------- Forwarded by Stinson Gibner/HOU/ECT on 06/29/2000\n","# 09:54 AM ---------------------------\n","\n","\n","# \tBarry Pearce @ ENRON COMMUNICATIONS\n","# \t06/29/2000 09:09 AM\n","\n","# To: Stinson Gibner/HOU/ECT@ECT, Dale Surbey/LON/ECT@ECT, Ted\n","# Murphy/HOU/ECT@ECT\n","# cc: Lou Casari/Enron Communications@Enron Communications, John Echols/Enron\n","# Communications@Enron Communications, Jim Fallon/Enron Communications@Enron\n","# Communications\n","# Subject: EBS VaR Transaction Policy\n","\n","# Hey you guys,\n","\n","# We are trying to implement a 'VaR transaction' policy - and would like your\n","# opinion.\n","\n","# This is going to be tough - because I'm not sure we have implemented a\n","# similiar policy across any of our other 'books' - that is - we looking to\n","# bring in all the accrual/operational assets as well as the MTM stuff\n","# (lambdas). To have a real-live 'configuration' of the system.\n","\n","# If assets/routes/servers etc...are added - what is the impact on the 'value'\n","# of the system and what it's worth.\n","\n","# John has attached a draft below - for your review and thoughts.\n","\n","# I can see how this works in a trading environment - when you actually know\n","# the VaR of your whole trading portfolio. However - I've not seen this done\n","# with a mixture of MTM & accrual assets. Add the spice of a 'operational\n","# system' valuation  - and this will be tough to quantify and model.\n","\n","# Step 1 - configure system and value it\n","# Step 2 - calculate a VaR on this. We will need to do some work here!\n","# Step 3 - calculate the incremental VaR of new deals/amendements/reconfigs etc\n","# - tough....\n","\n","# See what you think?\n","\n","# B.\n","\n","\n","\n","\n","\n","\n","# \tJohn Echols\n","# \t06/28/00 05:41 PM\n","\n","# \t\t To: Jim Fallon/Enron Communications@Enron Communications, Barry Pearce/Enron\n","# Communications@Enron Communications, Lou Casari/Enron Communications@Enron\n","# Communications\n","# \t\t cc:\n","# \t\t Subject: Policies\n","\n","# Here is a first rough draft of a \"value @ risk\" transaction policy.\n","\n","# I would like you to start looking at where we are going on the policy and get\n","# some early thinking going on limits for the V@R.  For example, should we\n","# effectively shut down all server relocations without approval,  or allow some\n","# level of MB of storage to be moved around or reconfigured?\n","\n","# I need some commercial and industry realism for this.  We may need Rick\n","# Paiste or your industry helpers (Marquardt, etc. to help us).\n","\n","# Barry,  Lou,  I need your input."],"metadata":{"id":"rFM-CCdcR-g5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","按照一系列关键词找还是有1万多封。现在打算试一下outlier分析来找。现在还是太多了。\n","\n","只按照一个关键词找是几百封， 要是这个词找的对好像有点戏能筛出5封。\n","\n","END of Yi Xia section."],"metadata":{"id":"Qyt_dXpAXM_U"}},{"cell_type":"markdown","source":["# END of Yi Xia section.\n","\n"],"metadata":{"id":"2YKgGP2Iod0b"}}]}